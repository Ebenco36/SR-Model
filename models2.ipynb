{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39383438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-31 03:59:11,643 - DEBUG - Verbose logging enabled\n",
      "2025-12-31 03:59:11,643 - INFO - ======================================================================\n",
      "2025-12-31 03:59:11,643 - INFO - NER EXTRACTION PIPELINE - 55 ENTITY TYPES\n",
      "2025-12-31 03:59:11,643 - INFO - ======================================================================\n",
      "2025-12-31 03:59:11,643 - INFO - Corpus: /Users/awotoroebenezer/Desktop/SenseBackend/infectious_disease_papers\n",
      "2025-12-31 03:59:11,643 - INFO - Output: /Users/awotoroebenezer/Desktop/SenseBackend/dataset_scaffold/infection_ner_llm.jsonl\n",
      "2025-12-31 03:59:11,643 - INFO - LLM: Enabled\n",
      "2025-12-31 03:59:11,643 - INFO - Workers: 1\n",
      "2025-12-31 03:59:11,643 - INFO - Resume: False\n",
      "2025-12-31 03:59:11,643 - INFO - Total entity types: 55\n",
      "2025-12-31 03:59:11,643 - INFO - ======================================================================\n",
      "2025-12-31 03:59:11,643 - INFO - Found 6 text files\n",
      "2025-12-31 03:59:11,643 - INFO - Files to process: 6\n",
      "2025-12-31 03:59:11,648 - INFO - Connected to Ollama. Available models: ['llama3.1:8b']\n",
      "2025-12-31 03:59:11,648 - INFO - Model llama3.1:8b is available\n",
      "2025-12-31 03:59:11,654 - INFO - [1/6] Processing: hiv_hpv_review.txt\n",
      "2025-12-31 03:59:11,654 - INFO - Processing document: hiv_hpv_review\n",
      "2025-12-31 03:59:11,656 - DEBUG - Rule-based extraction found 31 entities across 20 unique labels\n",
      "2025-12-31 03:59:11,656 - DEBUG - LLM attempt 1 for doc hiv_hpv_review\n",
      "2025-12-31 03:59:42,995 - INFO - LLM extracted 31 valid entities from hiv_hpv_review\n",
      "2025-12-31 03:59:42,997 - INFO - LLM found 31 entities for hiv_hpv_review\n",
      "2025-12-31 03:59:42,998 - INFO - Successfully processed hiv_hpv_review: 42 entities across 25 labels\n",
      "2025-12-31 03:59:42,999 - INFO - [2/6] Processing: malaria_modeling_review.txt\n",
      "2025-12-31 03:59:42,999 - INFO - Processing document: malaria_modeling_review\n",
      "2025-12-31 03:59:43,003 - DEBUG - Rule-based extraction found 22 entities across 15 unique labels\n",
      "2025-12-31 03:59:43,003 - DEBUG - LLM attempt 1 for doc malaria_modeling_review\n",
      "2025-12-31 03:59:56,019 - DEBUG - Invalid label 'INTERVENTIONS & PROGRAMS' for entity 'malaria vaccine deployment'\n",
      "2025-12-31 03:59:56,019 - DEBUG - Invalid label 'LIMITATION' for entity 'uncertainty in vaccine efficacy duration'\n",
      "2025-12-31 03:59:56,019 - DEBUG - Invalid label 'LIMITATION' for entity 'herd immunity effects'\n",
      "2025-12-31 03:59:56,019 - DEBUG - Invalid label 'INTERVENTIONS & PROGRAMS' for entity 'insecticide-treated nets'\n",
      "2025-12-31 03:59:56,019 - INFO - LLM extracted 25 valid entities from malaria_modeling_review\n",
      "2025-12-31 03:59:56,019 - DEBUG - Could not find exact match for 'systematic review' in text\n",
      "2025-12-31 03:59:56,019 - DEBUG - Could not find exact match for '42 studies' in text\n",
      "2025-12-31 03:59:56,019 - INFO - LLM found 23 entities for malaria_modeling_review\n",
      "2025-12-31 03:59:56,020 - INFO - Successfully processed malaria_modeling_review: 31 entities across 19 labels\n",
      "2025-12-31 03:59:56,020 - INFO - [3/6] Processing: paper1.txt\n",
      "2025-12-31 03:59:56,020 - INFO - Processing document: paper1\n",
      "2025-12-31 03:59:56,026 - DEBUG - Rule-based extraction found 24 entities across 11 unique labels\n",
      "2025-12-31 03:59:56,026 - DEBUG - LLM attempt 1 for doc paper1\n",
      "2025-12-31 04:00:10,588 - INFO - LLM extracted 34 valid entities from paper1\n",
      "2025-12-31 04:00:10,590 - DEBUG - Could not find exact match for '35,812 participants' in text\n",
      "2025-12-31 04:00:10,590 - DEBUG - Could not find exact match for '10,437 participants' in text\n",
      "2025-12-31 04:00:10,590 - DEBUG - Could not find exact match for '7,670 participants' in text\n",
      "2025-12-31 04:00:10,590 - DEBUG - Could not find exact match for '2,909 participants' in text\n",
      "2025-12-31 04:00:10,590 - DEBUG - Could not find exact match for '2,033 participants' in text\n",
      "2025-12-31 04:00:10,590 - DEBUG - Could not find exact match for '3,204 participants' in text\n",
      "2025-12-31 04:00:10,590 - DEBUG - Could not find exact match for '2,945 participants' in text\n",
      "2025-12-31 04:00:10,590 - INFO - LLM found 27 entities for paper1\n",
      "2025-12-31 04:00:10,590 - INFO - Successfully processed paper1: 34 entities across 12 labels\n",
      "2025-12-31 04:00:10,591 - INFO - [4/6] Processing: paper2.txt\n",
      "2025-12-31 04:00:10,591 - INFO - Processing document: paper2\n",
      "2025-12-31 04:00:10,591 - DEBUG - Rule-based extraction found 8 entities across 4 unique labels\n",
      "2025-12-31 04:00:10,591 - DEBUG - LLM attempt 1 for doc paper2\n",
      "2025-12-31 04:00:13,824 - DEBUG - Invalid label 'OUTCOME' for entity 'MDR-TB treatment outcomes'\n",
      "2025-12-31 04:00:13,825 - INFO - LLM extracted 7 valid entities from paper2\n",
      "2025-12-31 04:00:13,825 - INFO - LLM found 7 entities for paper2\n",
      "2025-12-31 04:00:13,825 - INFO - Successfully processed paper2: 10 entities across 5 labels\n",
      "2025-12-31 04:00:13,825 - INFO - [5/6] Processing: paper3.txt\n",
      "2025-12-31 04:00:13,825 - INFO - Processing document: paper3\n",
      "2025-12-31 04:00:13,826 - DEBUG - Rule-based extraction found 5 entities across 5 unique labels\n",
      "2025-12-31 04:00:13,826 - DEBUG - LLM attempt 1 for doc paper3\n",
      "2025-12-31 04:00:17,349 - INFO - LLM extracted 7 valid entities from paper3\n",
      "2025-12-31 04:00:17,349 - INFO - LLM found 7 entities for paper3\n",
      "2025-12-31 04:00:17,349 - INFO - Successfully processed paper3: 7 entities across 6 labels\n",
      "2025-12-31 04:00:17,349 - INFO - [6/6] Processing: tb_economic_review.txt\n",
      "2025-12-31 04:00:17,349 - INFO - Processing document: tb_economic_review\n",
      "2025-12-31 04:00:17,351 - DEBUG - Rule-based extraction found 21 entities across 12 unique labels\n",
      "2025-12-31 04:00:17,351 - DEBUG - LLM attempt 1 for doc tb_economic_review\n",
      "2025-12-31 04:00:28,629 - DEBUG - Invalid label 'OUTCOME_MEASURE' for entity 'DALY averted'\n",
      "2025-12-31 04:00:28,630 - DEBUG - Invalid label 'FACTORS_INFLUENCING_COST_EFFECTIVENESS' for entity 'price and efficacy'\n",
      "2025-12-31 04:00:28,630 - DEBUG - Invalid label 'ETHICAL_CONSIDERATIONS' for entity 'equitable access'\n",
      "2025-12-31 04:00:28,630 - DEBUG - Invalid label 'ETHICAL_CONSIDERATIONS' for entity 'program sustainability'\n",
      "2025-12-31 04:00:28,630 - INFO - LLM extracted 23 valid entities from tb_economic_review\n",
      "2025-12-31 04:00:28,630 - DEBUG - Could not find exact match for 'systematic review' in text\n",
      "2025-12-31 04:00:28,630 - DEBUG - Could not find exact match for '18 studies' in text\n",
      "2025-12-31 04:00:28,630 - INFO - LLM found 21 entities for tb_economic_review\n",
      "2025-12-31 04:00:28,631 - INFO - Successfully processed tb_economic_review: 31 entities across 17 labels\n",
      "2025-12-31 04:00:28,631 - INFO - ======================================================================\n",
      "2025-12-31 04:00:28,631 - INFO - PROCESSING COMPLETE\n",
      "2025-12-31 04:00:28,631 - INFO - ======================================================================\n",
      "2025-12-31 04:00:28,631 - INFO - Total time: 77.0 seconds\n",
      "2025-12-31 04:00:28,631 - INFO - Files processed: 6\n",
      "2025-12-31 04:00:28,631 - INFO - Files failed: 0\n",
      "2025-12-31 04:00:28,631 - INFO - Success rate: 100.0%\n",
      "2025-12-31 04:00:28,631 - INFO - Total entity types configured: 55\n",
      "2025-12-31 04:00:28,631 - INFO - Output file: /Users/awotoroebenezer/Desktop/SenseBackend/dataset_scaffold/infection_ner_llm.jsonl\n",
      "2025-12-31 04:00:28,632 - INFO - \n",
      "==================================================\n",
      "2025-12-31 04:00:28,632 - INFO - SAMPLE OUTPUT (first document):\n",
      "2025-12-31 04:00:28,632 - INFO - ==================================================\n",
      "2025-12-31 04:00:28,632 - INFO - Document: hiv_hpv_review\n",
      "2025-12-31 04:00:28,632 - INFO - Text preview: A systematic review of HPV vaccination in HIV-positive populations.\n",
      "\n",
      "We conducted a systematic review and meta-analysis of 25 studies from PubMed, MED...\n",
      "2025-12-31 04:00:28,632 - INFO - Total entities found: 31\n",
      "2025-12-31 04:00:28,632 - INFO - Unique labels found: 20\n",
      "2025-12-31 04:00:28,632 - INFO - \n",
      "Top 10 labels by frequency:\n",
      "2025-12-31 04:00:28,632 - INFO -   RISK_GROUP: 4\n",
      "2025-12-31 04:00:28,632 - INFO -   REVIEW_TYPE: 3\n",
      "2025-12-31 04:00:28,632 - INFO -   PATHOGEN: 3\n",
      "2025-12-31 04:00:28,632 - INFO -   DATABASE: 3\n",
      "2025-12-31 04:00:28,632 - INFO -   N_STUDIES: 2\n",
      "2025-12-31 04:00:28,632 - INFO -   REGION: 2\n",
      "2025-12-31 04:00:28,632 - INFO -   DATE_OF_LAST_LITERATURE_SEARCH: 1\n",
      "2025-12-31 04:00:28,632 - INFO -   VACCINE_TYPE: 1\n",
      "2025-12-31 04:00:28,632 - INFO -   SAMPLE_SIZE: 1\n",
      "2025-12-31 04:00:28,632 - INFO -   GENDER: 1\n",
      "2025-12-31 04:00:28,632 - INFO - \n",
      "First 5 entities:\n",
      "2025-12-31 04:00:28,632 - INFO -   1. [REVIEW_TYPE] 'systematic review' (pos: 2-19)\n",
      "2025-12-31 04:00:28,632 - INFO -   2. [PATHOGEN] 'HPV' (pos: 23-26)\n",
      "2025-12-31 04:00:28,632 - INFO -   3. [RISK_GROUP] 'HIV-positive' (pos: 42-54)\n",
      "2025-12-31 04:00:28,632 - INFO -   4. [REVIEW_TYPE] 'systematic review' (pos: 84-101)\n",
      "2025-12-31 04:00:28,632 - INFO -   5. [REVIEW_TYPE] 'meta-analysis' (pos: 106-119)\n",
      "2025-12-31 04:00:28,632 - INFO - \n",
      "Statistics:\n",
      "2025-12-31 04:00:28,632 - INFO -   Rule-based: 31\n",
      "2025-12-31 04:00:28,632 - INFO -   LLM: 0\n",
      "2025-12-31 04:00:28,632 - INFO -   Total: 31\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract entities from raw SR texts (PDF → TXT first if needed)\n",
    "!python -m src.SR.data_generation.ner_extractor --corpus infectious_disease_papers --out dataset_scaffold/infection_ner_llm.jsonl --llm --model llama3.1:8b --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ae3680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-31 04:04:48,361 - INFO - ============================================================\n",
      "2025-12-31 04:04:48,361 - INFO - QA DATASET GENERATION\n",
      "2025-12-31 04:04:48,361 - INFO - ============================================================\n",
      "2025-12-31 04:04:48,361 - INFO - Input NER file: /Users/awotoroebenezer/Desktop/SenseBackend/dataset_scaffold/infection_ner_llm.jsonl\n",
      "2025-12-31 04:04:48,361 - INFO - Output directory: /Users/awotoroebenezer/Desktop/SenseBackend/qa_dataset\n",
      "2025-12-31 04:04:48,361 - INFO - Train/Val/Test split: 0.70/0.15/0.15\n",
      "2025-12-31 04:04:48,361 - INFO - Random seed: 42\n",
      "2025-12-31 04:04:48,361 - INFO - ============================================================\n",
      "2025-12-31 04:04:48,361 - INFO - Loaded 12 NER results from dataset_scaffold/infection_ner_llm.jsonl\n",
      "2025-12-31 04:04:48,362 - INFO - Generated 20 QA pairs for hiv_hpv_review\n",
      "2025-12-31 04:04:48,362 - INFO - Generated 15 QA pairs for malaria_modeling_review\n",
      "2025-12-31 04:04:48,362 - INFO - Generated 11 QA pairs for paper1\n",
      "2025-12-31 04:04:48,362 - INFO - Generated 4 QA pairs for paper2\n",
      "2025-12-31 04:04:48,362 - INFO - Generated 5 QA pairs for paper3\n",
      "2025-12-31 04:04:48,362 - INFO - Generated 12 QA pairs for tb_economic_review\n",
      "2025-12-31 04:04:48,362 - INFO - Generated 25 QA pairs for hiv_hpv_review\n",
      "2025-12-31 04:04:48,362 - INFO - Generated 19 QA pairs for malaria_modeling_review\n",
      "2025-12-31 04:04:48,362 - INFO - Generated 12 QA pairs for paper1\n",
      "2025-12-31 04:04:48,362 - INFO - Generated 5 QA pairs for paper2\n",
      "2025-12-31 04:04:48,362 - INFO - Generated 6 QA pairs for paper3\n",
      "2025-12-31 04:04:48,362 - INFO - Generated 17 QA pairs for tb_economic_review\n",
      "2025-12-31 04:04:48,362 - INFO - Dataset split: Train=105, Val=22, Test=24\n",
      "2025-12-31 04:04:48,363 - INFO - Wrote 105 examples to qa_dataset/train/train.jsonl\n",
      "2025-12-31 04:04:48,364 - INFO - Created sample file qa_dataset/train/train.json with first 100 examples\n",
      "2025-12-31 04:04:48,365 - INFO - Wrote 22 examples to qa_dataset/validation/validation.jsonl\n",
      "2025-12-31 04:04:48,365 - INFO - Created sample file qa_dataset/validation/validation.json with first 100 examples\n",
      "2025-12-31 04:04:48,366 - INFO - Wrote 24 examples to qa_dataset/test/test.jsonl\n",
      "2025-12-31 04:04:48,366 - INFO - Created sample file qa_dataset/test/test.json with first 100 examples\n",
      "2025-12-31 04:04:48,366 - INFO - Dataset statistics saved to qa_dataset/dataset_statistics.json\n",
      "2025-12-31 04:04:48,366 - INFO - \n",
      "============================================================\n",
      "2025-12-31 04:04:48,366 - INFO - SAMPLE QA PAIRS (TRAIN SPLIT)\n",
      "2025-12-31 04:04:48,366 - INFO - ============================================================\n",
      "2025-12-31 04:04:48,366 - INFO - \n",
      "Example 1:\n",
      "2025-12-31 04:04:48,366 - INFO -   ID: tb_economic_review_q66_SEARCH_TERMS\n",
      "2025-12-31 04:04:48,366 - INFO -   Document: tb_economic_review\n",
      "2025-12-31 04:04:48,366 - INFO -   Question: What keywords or MeSH terms were used in the search?\n",
      "2025-12-31 04:04:48,366 - INFO -   Context preview: Rapid review of economic evaluations for tuberculosis vaccines.\n",
      "\n",
      "We performed a rapid review of 18 e...\n",
      "2025-12-31 04:04:48,366 - INFO -   Number of answers: 1\n",
      "2025-12-31 04:04:48,366 - INFO -   Answers:\n",
      "2025-12-31 04:04:48,366 - INFO -     1. 'Sensitivity analysis showed price and efficacy were most influential ' (position: 796)\n",
      "2025-12-31 04:04:48,366 - INFO -   Metadata: {'entity_label': 'SEARCH_TERMS', 'num_answers': 1, 'generation_date': '2025-12-31T04:04:48.362259'}\n",
      "2025-12-31 04:04:48,366 - INFO - \n",
      "Example 2:\n",
      "2025-12-31 04:04:48,366 - INFO -   ID: tb_economic_review_q135_REVIEW_TYPE\n",
      "2025-12-31 04:04:48,366 - INFO -   Document: tb_economic_review\n",
      "2025-12-31 04:04:48,366 - INFO -   Question: What is the study design type?\n",
      "2025-12-31 04:04:48,366 - INFO -   Context preview: Rapid review of economic evaluations for tuberculosis vaccines.\n",
      "\n",
      "We performed a rapid review of 18 e...\n",
      "2025-12-31 04:04:48,366 - INFO -   Number of answers: 2\n",
      "2025-12-31 04:04:48,366 - INFO -   Answers:\n",
      "2025-12-31 04:04:48,366 - INFO -     1. 'Rapid review' (position: 0)\n",
      "2025-12-31 04:04:48,366 - INFO -     2. 'rapid review' (position: 80)\n",
      "2025-12-31 04:04:48,366 - INFO -   Metadata: {'entity_label': 'REVIEW_TYPE', 'num_answers': 2, 'generation_date': '2025-12-31T04:04:48.362568'}\n",
      "2025-12-31 04:04:48,367 - INFO - \n",
      "Example 3:\n",
      "2025-12-31 04:04:48,367 - INFO -   ID: malaria_modeling_review_q99_EFFICACY\n",
      "2025-12-31 04:04:48,367 - INFO -   Document: malaria_modeling_review\n",
      "2025-12-31 04:04:48,367 - INFO -   Question: What effectiveness was assessed?\n",
      "2025-12-31 04:04:48,367 - INFO -   Context preview: Scoping review of mathematical modeling for malaria vaccine deployment.\n",
      "\n",
      "This scoping review include...\n",
      "2025-12-31 04:04:48,367 - INFO -   Number of answers: 1\n",
      "2025-12-31 04:04:48,367 - INFO -   Answers:\n",
      "2025-12-31 04:04:48,367 - INFO -     1. 'Vaccine efficacy' (position: 336)\n",
      "2025-12-31 04:04:48,367 - INFO -   Metadata: {'entity_label': 'EFFICACY', 'num_answers': 1, 'generation_date': '2025-12-31T04:04:48.362400'}\n",
      "2025-12-31 04:04:48,367 - INFO - \n",
      "Label distribution in train split:\n",
      "2025-12-31 04:04:48,367 - INFO -   REVIEW_TYPE: 9 examples\n",
      "2025-12-31 04:04:48,367 - INFO -   N_STUDIES: 6 examples\n",
      "2025-12-31 04:04:48,367 - INFO -   SEARCH_TERMS: 5 examples\n",
      "2025-12-31 04:04:48,367 - INFO -   REGION: 5 examples\n",
      "2025-12-31 04:04:48,367 - INFO -   COUNTRY: 5 examples\n",
      "2025-12-31 04:04:48,367 - INFO -   PATHOGEN: 5 examples\n",
      "2025-12-31 04:04:48,367 - INFO -   CI: 5 examples\n",
      "2025-12-31 04:04:48,367 - INFO -   EFFICACY: 4 examples\n",
      "2025-12-31 04:04:48,367 - INFO -   DATE_OF_LAST_LITERATURE_SEARCH: 4 examples\n",
      "2025-12-31 04:04:48,367 - INFO -   COST: 4 examples\n",
      "2025-12-31 04:04:48,367 - INFO -   ... and 26 more labels\n",
      "2025-12-31 04:04:48,367 - INFO - \n",
      "============================================================\n",
      "2025-12-31 04:04:48,367 - INFO - GENERATION COMPLETE\n",
      "2025-12-31 04:04:48,367 - INFO - ============================================================\n",
      "2025-12-31 04:04:48,367 - INFO - Dataset saved to: /Users/awotoroebenezer/Desktop/SenseBackend/qa_dataset\n",
      "2025-12-31 04:04:48,367 - INFO - Check the following directories:\n",
      "2025-12-31 04:04:48,367 - INFO -   - Training data: qa_dataset/train/\n",
      "2025-12-31 04:04:48,367 - INFO -   - Validation data: qa_dataset/validation/\n",
      "2025-12-31 04:04:48,367 - INFO -   - Test data: qa_dataset/test/\n",
      "2025-12-31 04:04:48,367 - INFO -   - Statistics: qa_dataset/dataset_statistics.json\n"
     ]
    }
   ],
   "source": [
    "!python -m src.SR.data_generation.qa_generator \\\n",
    "  --ner ./dataset_scaffold/infection_ner_llm.jsonl \\\n",
    "  --output qa_dataset \\\n",
    "  --train-split 0.7 --val-split 0.15 --test-split 0.15 \\\n",
    "  --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb2f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"doc_id\": \"hiv_hpv_review\", \"text\": \"A systematic review of HPV vaccination in HIV-positive populations.  We conducted a systematic review and meta-analysis of 25 studies from PubMed, MEDLINE, and Embase. The literature search was conducted up to December 2023. Studies were included if they reporte\n",
      "{\"doc_id\": \"malaria_modeling_review\", \"text\": \"Scoping review of mathematical modeling for malaria vaccine deployment.  This scoping review included 42 modeling studies from 2010-2022. Models included SEIR compartmental models (n=28) and agent-based models (n=14).  Studies were conducted in high-bur\n",
      "{\"doc_id\": \"paper1\", \"text\": \"A systematic review of antiretroviral therapy adherence in Sub-Saharan Africa. 42 studies with 15,000 HIV-positive patients were included. Adherence to ART was 78% (95% CI 72-84%). Barriers included stigma and distance to clinics. Methods: We searched PubMed, Scopus, an\n"
     ]
    }
   ],
   "source": [
    "!head -3 dataset_scaffold/infection_ner_llm.jsonl | sed 's/\\\\n/ /g' | cut -c1-300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9859db36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"tb_economic_review_q66_SEARCH_TERMS\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"doc_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"tb_economic_review\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"question\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"What keywords or MeSH terms were used in the search?\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"context\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Rapid review of economic evaluations for tuberculosis vaccines.\\n\\nWe performed a rapid review of 18 economic evaluations published between 2015-2023.\\nDatabase searches included PubMed, Embase, and Web of Science using terms: \\\"tuberculosis vaccine\\\" AND (\\\"cost\\\" OR \\\"economic\\\").\\n\\nStudies were from low- and middle-income countries (LMICs) in WHO regions: AFRO, SEARO, and EMRO.\\nVaccine types included: M72/AS01E (efficacy 54%), VPM1002, and BCG revaccination.\\n\\nResults: Incremental cost-effectiveness ratios ranged from $120 to $2,500 per DALY averted.\\nVaccination was cost-saving in 8 studies and cost-effective in 10 studies (ICER < GDP per capita).\\n\\nFactors influencing cost-effectiveness: vaccine price ($1.50-$8.00 per dose), delivery strategy (school-based vs routine), and coverage (65%-90%).\\nSensitivity analysis showed price and efficacy were most influential parameters.\\n\\nEthical considerations included equitable access and program sustainability.\\nThis review provides evidence for investment in new TB vaccines in high-burden countries.\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"answers\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "      \u001b[0;32m\"Sensitivity analysis showed price and efficacy were most influential \"\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m]\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"answer_start\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "      \u001b[0;39m796\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m]\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"metadata\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[1;34m\"entity_label\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"SEARCH_TERMS\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"num_answers\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"generation_date\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"2025-12-31T04:04:48.362259\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"tb_economic_review_q135_REVIEW_TYPE\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"doc_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"tb_economic_review\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"question\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"What is the study design type?\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"context\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Rapid review of economic evaluations for tuberculosis vaccines.\\n\\nWe performed a rapid review of 18 economic evaluations published between 2015-2023.\\nDatabase searches included PubMed, Embase, and Web of Science using terms: \\\"tuberculosis vaccine\\\" AND (\\\"cost\\\" OR \\\"economic\\\").\\n\\nStudies were from low- and middle-income countries (LMICs) in WHO regions: AFRO, SEARO, and EMRO.\\nVaccine types included: M72/AS01E (efficacy 54%), VPM1002, and BCG revaccination.\\n\\nResults: Incremental cost-effectiveness ratios ranged from $120 to $2,500 per DALY averted.\\nVaccination was cost-saving in 8 studies and cost-effective in 10 studies (ICER < GDP per capita).\\n\\nFactors influencing cost-effectiveness: vaccine price ($1.50-$8.00 per dose), delivery strategy (school-based vs routine), and coverage (65%-90%).\\nSensitivity analysis showed price and efficacy were most influential parameters.\\n\\nEthical considerations included equitable access and program sustainability.\\nThis review provides evidence for investment in new TB vaccines in high-burden countries.\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"answers\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "      \u001b[0;32m\"Rapid review\"\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0;32m\"rapid review\"\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m]\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"answer_start\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "      \u001b[0;39m0\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0;39m80\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m]\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"metadata\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[1;34m\"entity_label\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"REVIEW_TYPE\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"num_answers\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"generation_date\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"2025-12-31T04:04:48.362568\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"malaria_modeling_review_q99_EFFICACY\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"doc_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"malaria_modeling_review\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"question\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"What effectiveness was assessed?\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"context\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Scoping review of mathematical modeling for malaria vaccine deployment.\\n\\nThis scoping review included 42 modeling studies from 2010-2022.\\nModels included SEIR compartmental models (n=28) and agent-based models (n=14).\\n\\nStudies were conducted in high-burden countries: Kenya, Uganda, Tanzania, Nigeria, and Democratic Republic of Congo.\\nVaccine efficacy assumptions ranged from 45% to 82% with duration of protection from 1 to 10 years.\\n\\nKey findings: Mass vaccination campaigns could reduce malaria incidence by 67% (95% CI 55-78%).\\nIncremental cost-effectiveness ratio was $150 per DALY averted in sub-Saharan Africa.\\nModel projections showed greatest impact in children under 5 years (RR 0.33, p<0.001).\\n\\nLimitations included uncertainty in vaccine efficacy duration and herd immunity effects.\\nFuture research should focus on integration with existing interventions like insecticide-treated nets.\\n\\nPRISMA guidelines were followed. Risk of bias was assessed using the JBI checklist.\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"answers\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "      \u001b[0;32m\"Vaccine efficacy\"\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m]\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"answer_start\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "      \u001b[0;39m336\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m]\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"metadata\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[1;34m\"entity_label\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"EFFICACY\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"num_answers\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"generation_date\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"2025-12-31T04:04:48.362400\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"qa_pairs_generated\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m151\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_REVIEW_TYPE\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m12\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_PATHOGEN\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m10\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_RISK_GROUP\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m4\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_N_STUDIES\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m9\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_DATABASE\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m6\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_DATE_OF_LAST_LITERATURE_SEARCH\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m5\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_VACCINE_TYPE\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m3\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_SAMPLE_SIZE\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m5\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_REGION\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m6\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_GENDER\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_EFFICACY\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m4\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_CI\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m8\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_IMMUNOGENICITY\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_SEARCH_TERMS\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m8\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_ECONOMIC\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m4\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_COST\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m4\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_QALY\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_LOGISTICS\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m3\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_ACCEPTANCE\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_QUALITY_TOOL\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m4\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"documents_processed\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m12\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"total_qa_pairs\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m151\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_MODELLING\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_COUNTRY\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m7\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_PROGRAM\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m3\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_ICER\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m4\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_AGE_GROUP\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m3\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_EFFECT_MEASURE\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m6\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_PVALUE\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_PERIOD\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_STUDY_DESIGN\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_WHO_REGION\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_FACILITATOR\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_TIMING\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_ETHICAL\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_PERCENT\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m5\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_DOSE\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_INCOME_GROUP\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"qa_COVERAGE\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"total_examples\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m151\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"train_examples\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m105\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"val_examples\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m22\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"test_examples\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m24\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"documents_with_qa\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m12\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"avg_qa_per_doc\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m12.583333333333334\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"split_ratios\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0.7\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"validation\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0.15\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0.15\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"generation_date\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"2025-12-31T04:04:48.366586\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!head -3 qa_dataset/train/train.jsonl | jq .\n",
    "!cat qa_dataset/dataset_statistics.json | jq .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd47eea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-31 13:15:58,085 - __main__ - INFO - Pipeline initialized\n",
      "2025-12-31 13:15:58,085 - __main__ - INFO -   Output: results\n",
      "2025-12-31 13:15:58,085 - __main__ - INFO -   Train: qa_dataset/train/train.jsonl\n",
      "2025-12-31 13:15:58,085 - __main__ - INFO -   Test: qa_dataset/test/test.jsonl\n",
      "2025-12-31 13:15:58,085 - __main__ - INFO -   Val: qa_dataset/validation/validation.jsonl\n",
      "2025-12-31 13:15:58,085 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:15:58,085 - __main__ - INFO - BiEncoder Training & Evaluation Pipeline\n",
      "2025-12-31 13:15:58,085 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:15:58,085 - __main__ - INFO - Output directory: results\n",
      "2025-12-31 13:15:58,085 - __main__ - INFO - Models to train: ['bert-base-cased', 'scibert', 'pubmedbert', 'biobert', 'deberta-v3-base']\n",
      "2025-12-31 13:15:58,095 - src.SR.BI.BiEncoder - INFO - ✓ MPS (Apple Silicon) available\n",
      "2025-12-31 13:15:58,095 - __main__ - INFO - Device: mps\n",
      "2025-12-31 13:15:58,095 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:15:58,095 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:15:58,095 - __main__ - INFO - TRAINING PHASE\n",
      "2025-12-31 13:15:58,095 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:15:58,095 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-31 13:15:58,095 - __main__ - INFO - TRAINING: bert-base-cased\n",
      "2025-12-31 13:15:58,095 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:15:58,095 - src.SR.BI.BiEncoder - INFO - ✓ MPS (Apple Silicon) available\n",
      "2025-12-31 13:15:58,097 - src.SR.BI.BiEncoderTrainer - INFO - Initializing BiEncoderTrainer\n",
      "2025-12-31 13:15:58,097 - src.SR.BI.BiEncoderTrainer - INFO -   Model: bert-base-cased\n",
      "2025-12-31 13:15:58,097 - src.SR.BI.BiEncoderTrainer - INFO -   Device: mps\n",
      "2025-12-31 13:15:58,097 - src.SR.BI.BiEncoder - INFO - Building BiEncoder: bert-base-cased\n",
      "2025-12-31 13:15:58,097 - src.SR.BI.BiEncoder - INFO -   HF name: bert-base-cased\n",
      "2025-12-31 13:15:58,097 - src.SR.BI.BiEncoder - INFO -   Device: mps\n",
      "2025-12-31 13:15:58,097 - src.SR.BI.BiEncoder - INFO - Loading transformer: bert-base-cased\n",
      "2025-12-31 13:15:58,727 - src.SR.BI.BiEncoder - INFO - Creating SentenceTransformer with pooling\n",
      "2025-12-31 13:15:59,050 - src.SR.BI.BiEncoder - INFO - ✓ BiEncoder built successfully (dim=768, device=mps)\n",
      "2025-12-31 13:15:59,050 - src.SR.BI.BiEncoderTrainer - INFO - Loading datasets...\n",
      "2025-12-31 13:15:59,052 - src.SR.BI.QADataset - INFO - Loaded 105 QA examples from qa_dataset/train/train.jsonl\n",
      "2025-12-31 13:15:59,052 - src.SR.BI.BiEncoderTrainer - INFO - Setting up validation evaluator...\n",
      "2025-12-31 13:15:59,053 - src.SR.BI.QADataset - INFO - Loaded 22 QA examples from qa_dataset/validation/validation.jsonl\n",
      "2025-12-31 13:15:59,053 - src.SR.BI.BiEncoderTrainer - INFO -   Evaluator: 22 queries, 6 docs\n",
      "2025-12-31 13:15:59,053 - src.SR.BI.BiEncoderTrainer - INFO - ✓ Validation evaluator ready\n",
      "2025-12-31 13:15:59,053 - src.SR.BI.BiEncoderTrainer - INFO - ================================================================================\n",
      "2025-12-31 13:15:59,053 - src.SR.BI.BiEncoderTrainer - INFO - TRAINING: bert-base-cased\n",
      "2025-12-31 13:15:59,053 - src.SR.BI.BiEncoderTrainer - INFO - ================================================================================\n",
      "2025-12-31 13:15:59,053 - src.SR.BI.BiEncoderTrainer - INFO - ✓ Validation active: Will save BEST model\n",
      "Computing widget examples:   0%|                     | 0/1 [00:00<?, ?example/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 33%|██████████████▋                             | 4/12 [00:09<00:17,  2.21s/it]2025-12-31 13:16:08,417 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Information Retrieval Evaluation of the model on the val_bert-base-cased dataset in epoch 1.0 after 4 steps:\n",
      "2025-12-31 13:16:09,424 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Queries: 22\n",
      "2025-12-31 13:16:09,424 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Corpus: 6\n",
      "\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Score-Function: cosine\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@1: 31.82%\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@3: 59.09%\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@5: 100.00%\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@10: 100.00%\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@1: 31.82%\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@3: 19.70%\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@5: 20.00%\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@10: 10.00%\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@1: 31.82%\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@3: 59.09%\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@5: 100.00%\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@10: 100.00%\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MRR@10: 0.5273\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - NDCG@10: 0.6426\n",
      "2025-12-31 13:16:09,427 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MAP@100: 0.5273\n",
      "2025-12-31 13:16:09,429 - sentence_transformers.SentenceTransformer - INFO - Save model to results/models/bert-base-cased\n",
      " 67%|█████████████████████████████▎              | 8/12 [00:37<00:25,  6.29s/it]2025-12-31 13:16:36,740 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Information Retrieval Evaluation of the model on the val_bert-base-cased dataset in epoch 2.0 after 8 steps:\n",
      "2025-12-31 13:16:36,938 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Queries: 22\n",
      "2025-12-31 13:16:36,938 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Corpus: 6\n",
      "\n",
      "2025-12-31 13:16:36,948 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Score-Function: cosine\n",
      "2025-12-31 13:16:36,949 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@1: 27.27%\n",
      "2025-12-31 13:16:36,950 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@3: 86.36%\n",
      "2025-12-31 13:16:36,950 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@5: 100.00%\n",
      "2025-12-31 13:16:36,950 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@10: 100.00%\n",
      "2025-12-31 13:16:36,950 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@1: 27.27%\n",
      "2025-12-31 13:16:36,950 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@3: 28.79%\n",
      "2025-12-31 13:16:36,950 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@5: 20.00%\n",
      "2025-12-31 13:16:36,950 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@10: 10.00%\n",
      "2025-12-31 13:16:36,951 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@1: 27.27%\n",
      "2025-12-31 13:16:36,951 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@3: 86.36%\n",
      "2025-12-31 13:16:36,951 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@5: 100.00%\n",
      "2025-12-31 13:16:36,951 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@10: 100.00%\n",
      "2025-12-31 13:16:36,951 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MRR@10: 0.5492\n",
      "2025-12-31 13:16:36,951 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - NDCG@10: 0.6626\n",
      "2025-12-31 13:16:36,951 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MAP@100: 0.5492\n",
      "2025-12-31 13:16:36,969 - sentence_transformers.SentenceTransformer - INFO - Save model to results/models/bert-base-cased\n",
      "100%|███████████████████████████████████████████| 12/12 [01:28<00:00,  9.79s/it]2025-12-31 13:17:27,849 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Information Retrieval Evaluation of the model on the val_bert-base-cased dataset in epoch 3.0 after 12 steps:\n",
      "2025-12-31 13:17:28,409 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Queries: 22\n",
      "2025-12-31 13:17:28,411 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Corpus: 6\n",
      "\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Score-Function: cosine\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@1: 27.27%\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@3: 90.91%\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@5: 100.00%\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@10: 100.00%\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@1: 27.27%\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@3: 30.30%\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@5: 20.00%\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@10: 10.00%\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@1: 27.27%\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@3: 90.91%\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@5: 100.00%\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@10: 100.00%\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MRR@10: 0.5530\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - NDCG@10: 0.6658\n",
      "2025-12-31 13:17:28,428 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MAP@100: 0.5530\n",
      "2025-12-31 13:17:28,431 - sentence_transformers.SentenceTransformer - INFO - Save model to results/models/bert-base-cased\n",
      "{'train_runtime': 89.7505, 'train_samples_per_second': 3.51, 'train_steps_per_second': 0.134, 'train_loss': 3.2342564264933267, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████████████| 12/12 [01:29<00:00,  7.48s/it]\n",
      "2025-12-31 13:17:28,951 - src.SR.BI.BiEncoderTrainer - INFO - ✓ Training completed in 89.9s\n",
      "2025-12-31 13:17:28,957 - __main__ - INFO - ✓ bert-base-cased training completed\n",
      "2025-12-31 13:17:28,957 - __main__ - INFO -   Model saved to: results/models/bert-base-cased\n",
      "2025-12-31 13:17:28,958 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-31 13:17:28,958 - __main__ - INFO - TRAINING: scibert\n",
      "2025-12-31 13:17:28,958 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:17:28,961 - src.SR.BI.BiEncoder - INFO - ✓ MPS (Apple Silicon) available\n",
      "2025-12-31 13:17:28,976 - src.SR.BI.BiEncoderTrainer - INFO - Initializing BiEncoderTrainer\n",
      "2025-12-31 13:17:28,976 - src.SR.BI.BiEncoderTrainer - INFO -   Model: scibert\n",
      "2025-12-31 13:17:28,976 - src.SR.BI.BiEncoderTrainer - INFO -   Device: mps\n",
      "2025-12-31 13:17:28,977 - src.SR.BI.BiEncoder - INFO - Building BiEncoder: scibert\n",
      "2025-12-31 13:17:28,977 - src.SR.BI.BiEncoder - INFO -   HF name: allenai/scibert_scivocab_uncased\n",
      "2025-12-31 13:17:28,977 - src.SR.BI.BiEncoder - INFO -   Device: mps\n",
      "2025-12-31 13:17:28,977 - src.SR.BI.BiEncoder - INFO - Loading transformer: allenai/scibert_scivocab_uncased\n",
      "2025-12-31 13:17:30,361 - src.SR.BI.BiEncoder - INFO - Creating SentenceTransformer with pooling\n",
      "2025-12-31 13:17:32,971 - src.SR.BI.BiEncoder - INFO - ✓ BiEncoder built successfully (dim=768, device=mps)\n",
      "2025-12-31 13:17:32,971 - src.SR.BI.BiEncoderTrainer - INFO - Loading datasets...\n",
      "2025-12-31 13:17:32,973 - src.SR.BI.QADataset - INFO - Loaded 105 QA examples from qa_dataset/train/train.jsonl\n",
      "2025-12-31 13:17:32,973 - src.SR.BI.BiEncoderTrainer - INFO - Setting up validation evaluator...\n",
      "2025-12-31 13:17:32,974 - src.SR.BI.QADataset - INFO - Loaded 22 QA examples from qa_dataset/validation/validation.jsonl\n",
      "2025-12-31 13:17:32,974 - src.SR.BI.BiEncoderTrainer - INFO -   Evaluator: 22 queries, 6 docs\n",
      "2025-12-31 13:17:32,974 - src.SR.BI.BiEncoderTrainer - INFO - ✓ Validation evaluator ready\n",
      "2025-12-31 13:17:32,976 - src.SR.BI.BiEncoderTrainer - INFO - ================================================================================\n",
      "2025-12-31 13:17:32,976 - src.SR.BI.BiEncoderTrainer - INFO - TRAINING: scibert\n",
      "2025-12-31 13:17:32,976 - src.SR.BI.BiEncoderTrainer - INFO - ================================================================================\n",
      "2025-12-31 13:17:32,977 - src.SR.BI.BiEncoderTrainer - INFO - ✓ Validation active: Will save BEST model\n",
      " 33%|██████████████▋                             | 4/12 [00:08<00:13,  1.75s/it]2025-12-31 13:17:42,450 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Information Retrieval Evaluation of the model on the val_scibert dataset in epoch 1.0 after 4 steps:\n",
      "2025-12-31 13:17:43,785 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Queries: 22\n",
      "2025-12-31 13:17:43,785 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Corpus: 6\n",
      "\n",
      "2025-12-31 13:17:43,790 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Score-Function: cosine\n",
      "2025-12-31 13:17:43,790 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@1: 36.36%\n",
      "2025-12-31 13:17:43,790 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@3: 68.18%\n",
      "2025-12-31 13:17:43,790 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@5: 95.45%\n",
      "2025-12-31 13:17:43,790 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@10: 100.00%\n",
      "2025-12-31 13:17:43,790 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@1: 36.36%\n",
      "2025-12-31 13:17:43,790 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@3: 22.73%\n",
      "2025-12-31 13:17:43,790 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@5: 19.09%\n",
      "2025-12-31 13:17:43,790 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@10: 10.00%\n",
      "2025-12-31 13:17:43,790 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@1: 36.36%\n",
      "2025-12-31 13:17:43,790 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@3: 68.18%\n",
      "2025-12-31 13:17:43,791 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@5: 95.45%\n",
      "2025-12-31 13:17:43,791 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@10: 100.00%\n",
      "2025-12-31 13:17:43,791 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MRR@10: 0.5636\n",
      "2025-12-31 13:17:43,791 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - NDCG@10: 0.6702\n",
      "2025-12-31 13:17:43,791 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MAP@100: 0.5636\n",
      "2025-12-31 13:17:43,794 - sentence_transformers.SentenceTransformer - INFO - Save model to results/models/scibert\n",
      " 67%|█████████████████████████████▎              | 8/12 [00:35<00:18,  4.52s/it]2025-12-31 13:18:08,734 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Information Retrieval Evaluation of the model on the val_scibert dataset in epoch 2.0 after 8 steps:\n",
      "2025-12-31 13:18:09,244 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Queries: 22\n",
      "2025-12-31 13:18:09,244 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Corpus: 6\n",
      "\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Score-Function: cosine\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@1: 27.27%\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@3: 68.18%\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@5: 95.45%\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@10: 100.00%\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@1: 27.27%\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@3: 22.73%\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@5: 19.09%\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@10: 10.00%\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@1: 27.27%\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@3: 68.18%\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@5: 95.45%\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@10: 100.00%\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MRR@10: 0.5136\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - NDCG@10: 0.6327\n",
      "2025-12-31 13:18:09,249 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MAP@100: 0.5136\n",
      "100%|███████████████████████████████████████████| 12/12 [00:45<00:00,  2.73s/it]2025-12-31 13:18:18,780 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Information Retrieval Evaluation of the model on the val_scibert dataset in epoch 3.0 after 12 steps:\n",
      "2025-12-31 13:18:19,218 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Queries: 22\n",
      "2025-12-31 13:18:19,218 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Corpus: 6\n",
      "\n",
      "2025-12-31 13:18:19,219 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Score-Function: cosine\n",
      "2025-12-31 13:18:19,219 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@1: 18.18%\n",
      "2025-12-31 13:18:19,219 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@3: 63.64%\n",
      "2025-12-31 13:18:19,219 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@5: 95.45%\n",
      "2025-12-31 13:18:19,219 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@10: 100.00%\n",
      "2025-12-31 13:18:19,219 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@1: 18.18%\n",
      "2025-12-31 13:18:19,220 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@3: 21.21%\n",
      "2025-12-31 13:18:19,220 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@5: 19.09%\n",
      "2025-12-31 13:18:19,220 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@10: 10.00%\n",
      "2025-12-31 13:18:19,220 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@1: 18.18%\n",
      "2025-12-31 13:18:19,220 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@3: 63.64%\n",
      "2025-12-31 13:18:19,220 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@5: 95.45%\n",
      "2025-12-31 13:18:19,220 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@10: 100.00%\n",
      "2025-12-31 13:18:19,220 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MRR@10: 0.4545\n",
      "2025-12-31 13:18:19,220 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - NDCG@10: 0.5881\n",
      "2025-12-31 13:18:19,220 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MAP@100: 0.4545\n",
      "{'train_runtime': 45.5203, 'train_samples_per_second': 6.92, 'train_steps_per_second': 0.264, 'train_loss': 3.270491282145182, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████████████| 12/12 [00:45<00:00,  3.79s/it]\n",
      "2025-12-31 13:18:19,236 - src.SR.BI.BiEncoderTrainer - INFO - ✓ Training completed in 46.3s\n",
      "2025-12-31 13:18:19,241 - __main__ - INFO - ✓ scibert training completed\n",
      "2025-12-31 13:18:19,241 - __main__ - INFO -   Model saved to: results/models/scibert\n",
      "2025-12-31 13:18:19,241 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-31 13:18:19,241 - __main__ - INFO - TRAINING: pubmedbert\n",
      "2025-12-31 13:18:19,241 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:18:19,241 - src.SR.BI.BiEncoder - INFO - ✓ MPS (Apple Silicon) available\n",
      "2025-12-31 13:18:19,247 - src.SR.BI.BiEncoderTrainer - INFO - Initializing BiEncoderTrainer\n",
      "2025-12-31 13:18:19,247 - src.SR.BI.BiEncoderTrainer - INFO -   Model: pubmedbert\n",
      "2025-12-31 13:18:19,247 - src.SR.BI.BiEncoderTrainer - INFO -   Device: mps\n",
      "2025-12-31 13:18:19,247 - src.SR.BI.BiEncoder - INFO - Building BiEncoder: pubmedbert\n",
      "2025-12-31 13:18:19,248 - src.SR.BI.BiEncoder - INFO -   HF name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstracts\n",
      "2025-12-31 13:18:19,248 - src.SR.BI.BiEncoder - INFO -   Device: mps\n",
      "2025-12-31 13:18:19,248 - src.SR.BI.BiEncoder - INFO - Loading transformer: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstracts\n",
      "2025-12-31 13:18:19,400 - src.SR.BI.BiEncoder - ERROR - Failed to load transformer: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstracts is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "2025-12-31 13:18:19,400 - __main__ - ERROR - ✗ pubmedbert training failed: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstracts is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "2025-12-31 13:18:19,400 - __main__ - ERROR - microsoft/BiomedNLP-PubMedBERT-base-uncased-abstracts is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstracts/resolve/main/adapter_config.json\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/utils/hub.py\", line 424, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1486, in _raise_on_head_call_error\n",
      "    raise head_call_error\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1376, in _get_metadata_or_catch_error\n",
      "    metadata = get_hf_file_metadata(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1296, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 280, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 304, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 458, in hf_raise_for_status\n",
      "    raise _format(RepositoryNotFoundError, message, response) from e\n",
      "huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6955148b-132327a76a2f5dd21e664421;db8eccfd-8048-47d6-b699-2f0e43605e6b)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstracts/resolve/main/adapter_config.json.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/src/SR/BI/BiEncoderPipeline.py\", line 118, in train_models\n",
      "    trainer = BiEncoderTrainer(\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/src/SR/BI/BiEncoderTrainer.py\", line 86, in __init__\n",
      "    self.encoder = BiEncoder(model_key, device=self.device)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/src/SR/BI/BiEncoder.py\", line 74, in __init__\n",
      "    self._build_model()\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/src/SR/BI/BiEncoder.py\", line 86, in _build_model\n",
      "    word_embedding_model = models.Transformer(\n",
      "                           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py\", line 80, in __init__\n",
      "    config, is_peft_model = self._load_config(model_name_or_path, cache_dir, backend, config_args)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py\", line 121, in _load_config\n",
      "    find_adapter_config_file(\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/utils/peft_utils.py\", line 88, in find_adapter_config_file\n",
      "    adapter_cached_filename = cached_file(\n",
      "                              ^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/utils/hub.py\", line 266, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/utils/hub.py\", line 456, in cached_files\n",
      "    raise EnvironmentError(\n",
      "OSError: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstracts is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "2025-12-31 13:18:19,416 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-31 13:18:19,416 - __main__ - INFO - TRAINING: biobert\n",
      "2025-12-31 13:18:19,416 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:18:19,416 - src.SR.BI.BiEncoder - INFO - ✓ MPS (Apple Silicon) available\n",
      "2025-12-31 13:18:19,416 - src.SR.BI.BiEncoderTrainer - INFO - Initializing BiEncoderTrainer\n",
      "2025-12-31 13:18:19,416 - src.SR.BI.BiEncoderTrainer - INFO -   Model: biobert\n",
      "2025-12-31 13:18:19,416 - src.SR.BI.BiEncoderTrainer - INFO -   Device: mps\n",
      "2025-12-31 13:18:19,416 - src.SR.BI.BiEncoder - INFO - Building BiEncoder: biobert\n",
      "2025-12-31 13:18:19,416 - src.SR.BI.BiEncoder - INFO -   HF name: dmis-lab/biobert-base-cased-v1.1\n",
      "2025-12-31 13:18:19,416 - src.SR.BI.BiEncoder - INFO -   Device: mps\n",
      "2025-12-31 13:18:19,417 - src.SR.BI.BiEncoder - INFO - Loading transformer: dmis-lab/biobert-base-cased-v1.1\n",
      "2025-12-31 13:18:20,831 - src.SR.BI.BiEncoder - INFO - Creating SentenceTransformer with pooling\n",
      "2025-12-31 13:18:21,734 - src.SR.BI.BiEncoder - INFO - ✓ BiEncoder built successfully (dim=768, device=mps)\n",
      "2025-12-31 13:18:21,734 - src.SR.BI.BiEncoderTrainer - INFO - Loading datasets...\n",
      "2025-12-31 13:18:21,736 - src.SR.BI.QADataset - INFO - Loaded 105 QA examples from qa_dataset/train/train.jsonl\n",
      "2025-12-31 13:18:21,736 - src.SR.BI.BiEncoderTrainer - INFO - Setting up validation evaluator...\n",
      "2025-12-31 13:18:21,737 - src.SR.BI.QADataset - INFO - Loaded 22 QA examples from qa_dataset/validation/validation.jsonl\n",
      "2025-12-31 13:18:21,737 - src.SR.BI.BiEncoderTrainer - INFO -   Evaluator: 22 queries, 6 docs\n",
      "2025-12-31 13:18:21,737 - src.SR.BI.BiEncoderTrainer - INFO - ✓ Validation evaluator ready\n",
      "2025-12-31 13:18:21,739 - src.SR.BI.BiEncoderTrainer - INFO - ================================================================================\n",
      "2025-12-31 13:18:21,739 - src.SR.BI.BiEncoderTrainer - INFO - TRAINING: biobert\n",
      "2025-12-31 13:18:21,739 - src.SR.BI.BiEncoderTrainer - INFO - ================================================================================\n",
      "2025-12-31 13:18:21,740 - src.SR.BI.BiEncoderTrainer - INFO - ✓ Validation active: Will save BEST model\n",
      " 33%|██████████████▋                             | 4/12 [00:45<01:42, 12.75s/it]2025-12-31 13:19:08,429 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Information Retrieval Evaluation of the model on the val_biobert dataset in epoch 1.0 after 4 steps:\n",
      "2025-12-31 13:19:08,690 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Queries: 22\n",
      "2025-12-31 13:19:08,690 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Corpus: 6\n",
      "\n",
      "2025-12-31 13:19:08,694 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Score-Function: cosine\n",
      "2025-12-31 13:19:08,694 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@1: 27.27%\n",
      "2025-12-31 13:19:08,694 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@3: 72.73%\n",
      "2025-12-31 13:19:08,694 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@5: 95.45%\n",
      "2025-12-31 13:19:08,694 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@10: 100.00%\n",
      "2025-12-31 13:19:08,694 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@1: 27.27%\n",
      "2025-12-31 13:19:08,694 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@3: 24.24%\n",
      "2025-12-31 13:19:08,694 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@5: 19.09%\n",
      "2025-12-31 13:19:08,694 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@10: 10.00%\n",
      "2025-12-31 13:19:08,694 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@1: 27.27%\n",
      "2025-12-31 13:19:08,695 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@3: 72.73%\n",
      "2025-12-31 13:19:08,695 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@5: 95.45%\n",
      "2025-12-31 13:19:08,695 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@10: 100.00%\n",
      "2025-12-31 13:19:08,695 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MRR@10: 0.5318\n",
      "2025-12-31 13:19:08,695 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - NDCG@10: 0.6478\n",
      "2025-12-31 13:19:08,695 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MAP@100: 0.5318\n",
      "2025-12-31 13:19:08,698 - sentence_transformers.SentenceTransformer - INFO - Save model to results/models/biobert\n",
      " 67%|█████████████████████████████▎              | 8/12 [04:51<03:05, 46.37s/it]2025-12-31 13:23:14,290 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Information Retrieval Evaluation of the model on the val_biobert dataset in epoch 2.0 after 8 steps:\n",
      "2025-12-31 13:23:14,543 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Queries: 22\n",
      "2025-12-31 13:23:14,543 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Corpus: 6\n",
      "\n",
      "2025-12-31 13:23:14,547 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Score-Function: cosine\n",
      "2025-12-31 13:23:14,547 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@1: 22.73%\n",
      "2025-12-31 13:23:14,547 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@3: 68.18%\n",
      "2025-12-31 13:23:14,547 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@5: 90.91%\n",
      "2025-12-31 13:23:14,548 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@10: 100.00%\n",
      "2025-12-31 13:23:14,548 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@1: 22.73%\n",
      "2025-12-31 13:23:14,548 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@3: 22.73%\n",
      "2025-12-31 13:23:14,548 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@5: 18.18%\n",
      "2025-12-31 13:23:14,548 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@10: 10.00%\n",
      "2025-12-31 13:23:14,548 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@1: 22.73%\n",
      "2025-12-31 13:23:14,548 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@3: 68.18%\n",
      "2025-12-31 13:23:14,548 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@5: 90.91%\n",
      "2025-12-31 13:23:14,548 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@10: 100.00%\n",
      "2025-12-31 13:23:14,548 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MRR@10: 0.4614\n",
      "2025-12-31 13:23:14,548 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - NDCG@10: 0.5927\n",
      "2025-12-31 13:23:14,548 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MAP@100: 0.4614\n",
      "100%|███████████████████████████████████████████| 12/12 [11:17<00:00, 82.59s/it]2025-12-31 13:29:40,299 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Information Retrieval Evaluation of the model on the val_biobert dataset in epoch 3.0 after 12 steps:\n",
      "2025-12-31 13:29:40,616 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Queries: 22\n",
      "2025-12-31 13:29:40,617 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Corpus: 6\n",
      "\n",
      "2025-12-31 13:29:40,624 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Score-Function: cosine\n",
      "2025-12-31 13:29:40,624 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@1: 27.27%\n",
      "2025-12-31 13:29:40,624 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@3: 54.55%\n",
      "2025-12-31 13:29:40,624 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@5: 90.91%\n",
      "2025-12-31 13:29:40,624 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Accuracy@10: 100.00%\n",
      "2025-12-31 13:29:40,625 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@1: 27.27%\n",
      "2025-12-31 13:29:40,625 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@3: 18.18%\n",
      "2025-12-31 13:29:40,625 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@5: 18.18%\n",
      "2025-12-31 13:29:40,625 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Precision@10: 10.00%\n",
      "2025-12-31 13:29:40,625 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@1: 27.27%\n",
      "2025-12-31 13:29:40,625 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@3: 54.55%\n",
      "2025-12-31 13:29:40,625 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@5: 90.91%\n",
      "2025-12-31 13:29:40,625 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - Recall@10: 100.00%\n",
      "2025-12-31 13:29:40,625 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MRR@10: 0.4727\n",
      "2025-12-31 13:29:40,625 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - NDCG@10: 0.6001\n",
      "2025-12-31 13:29:40,625 - sentence_transformers.evaluation.InformationRetrievalEvaluator - INFO - MAP@100: 0.4727\n",
      "{'train_runtime': 677.6758, 'train_samples_per_second': 0.465, 'train_steps_per_second': 0.018, 'train_loss': 3.1236292521158853, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████████████| 12/12 [11:17<00:00, 56.47s/it]\n",
      "2025-12-31 13:29:40,652 - src.SR.BI.BiEncoderTrainer - INFO - ✓ Training completed in 678.9s\n",
      "2025-12-31 13:29:40,655 - __main__ - INFO - ✓ biobert training completed\n",
      "2025-12-31 13:29:40,655 - __main__ - INFO -   Model saved to: results/models/biobert\n",
      "2025-12-31 13:29:40,656 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-31 13:29:40,656 - __main__ - INFO - TRAINING: deberta-v3-base\n",
      "2025-12-31 13:29:40,656 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:29:40,659 - src.SR.BI.BiEncoder - INFO - ✓ MPS (Apple Silicon) available\n",
      "2025-12-31 13:29:40,673 - src.SR.BI.BiEncoderTrainer - INFO - Initializing BiEncoderTrainer\n",
      "2025-12-31 13:29:40,673 - src.SR.BI.BiEncoderTrainer - INFO -   Model: deberta-v3-base\n",
      "2025-12-31 13:29:40,673 - src.SR.BI.BiEncoderTrainer - INFO -   Device: mps\n",
      "2025-12-31 13:29:40,673 - src.SR.BI.BiEncoder - INFO - Building BiEncoder: deberta-v3-base\n",
      "2025-12-31 13:29:40,674 - src.SR.BI.BiEncoder - INFO -   HF name: microsoft/deberta-v3-base\n",
      "2025-12-31 13:29:40,674 - src.SR.BI.BiEncoder - INFO -   Device: mps\n",
      "2025-12-31 13:29:40,674 - src.SR.BI.BiEncoder - INFO - Loading transformer: microsoft/deberta-v3-base\n",
      "2025-12-31 13:29:42,092 - src.SR.BI.BiEncoder - ERROR - Failed to load transformer: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n",
      "2025-12-31 13:29:42,093 - __main__ - ERROR - ✗ deberta-v3-base training failed: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n",
      "2025-12-31 13:29:42,095 - __main__ - ERROR - Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py\", line 1587, in extract_vocab_merges_from_model\n",
      "    from tiktoken.load import load_tiktoken_bpe\n",
      "ModuleNotFoundError: No module named 'tiktoken'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py\", line 1730, in convert_slow_tokenizer\n",
      "    ).converted()\n",
      "      ^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py\", line 1624, in converted\n",
      "    tokenizer = self.tokenizer()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py\", line 1617, in tokenizer\n",
      "    vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py\", line 1589, in extract_vocab_merges_from_model\n",
      "    raise ValueError(\n",
      "ValueError: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/src/SR/BI/BiEncoderPipeline.py\", line 118, in train_models\n",
      "    trainer = BiEncoderTrainer(\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/src/SR/BI/BiEncoderTrainer.py\", line 86, in __init__\n",
      "    self.encoder = BiEncoder(model_key, device=self.device)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/src/SR/BI/BiEncoder.py\", line 74, in __init__\n",
      "    self._build_model()\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/src/SR/BI/BiEncoder.py\", line 86, in _build_model\n",
      "    word_embedding_model = models.Transformer(\n",
      "                           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py\", line 85, in __init__\n",
      "    self.tokenizer = AutoTokenizer.from_pretrained(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py\", line 992, in from_pretrained\n",
      "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2062, in from_pretrained\n",
      "    return cls._from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2302, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py\", line 103, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py\", line 139, in __init__\n",
      "    fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py\", line 1732, in convert_slow_tokenizer\n",
      "    raise ValueError(\n",
      "ValueError: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n",
      "2025-12-31 13:29:42,122 - __main__ - INFO - \n",
      "2025-12-31 13:29:42,122 - __main__ - INFO - Successfully trained: ['bert-base-cased', 'scibert', 'biobert']\n",
      "2025-12-31 13:29:42,125 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:29:42,125 - __main__ - INFO - EVALUATION PHASE\n",
      "2025-12-31 13:29:42,125 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:29:42,125 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-31 13:29:42,125 - __main__ - INFO - EVALUATING: bert-base-cased\n",
      "2025-12-31 13:29:42,125 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:29:42,125 - __main__ - ERROR - ✗ bert-base-cased evaluation failed: BiEncoderEvaluator.__init__() missing 2 required positional arguments: 'model_key' and 'test_jsonl'\n",
      "2025-12-31 13:29:42,125 - __main__ - ERROR - BiEncoderEvaluator.__init__() missing 2 required positional arguments: 'model_key' and 'test_jsonl'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/src/SR/BI/BiEncoderPipeline.py\", line 169, in evaluate_models\n",
      "    evaluator = BiEncoderEvaluator(\n",
      "                ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: BiEncoderEvaluator.__init__() missing 2 required positional arguments: 'model_key' and 'test_jsonl'\n",
      "2025-12-31 13:29:42,125 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-31 13:29:42,125 - __main__ - INFO - EVALUATING: scibert\n",
      "2025-12-31 13:29:42,125 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:29:42,125 - __main__ - ERROR - ✗ scibert evaluation failed: BiEncoderEvaluator.__init__() missing 2 required positional arguments: 'model_key' and 'test_jsonl'\n",
      "2025-12-31 13:29:42,125 - __main__ - ERROR - BiEncoderEvaluator.__init__() missing 2 required positional arguments: 'model_key' and 'test_jsonl'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/src/SR/BI/BiEncoderPipeline.py\", line 169, in evaluate_models\n",
      "    evaluator = BiEncoderEvaluator(\n",
      "                ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: BiEncoderEvaluator.__init__() missing 2 required positional arguments: 'model_key' and 'test_jsonl'\n",
      "2025-12-31 13:29:42,125 - __main__ - INFO - \n",
      "================================================================================\n",
      "2025-12-31 13:29:42,125 - __main__ - INFO - EVALUATING: biobert\n",
      "2025-12-31 13:29:42,125 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:29:42,125 - __main__ - ERROR - ✗ biobert evaluation failed: BiEncoderEvaluator.__init__() missing 2 required positional arguments: 'model_key' and 'test_jsonl'\n",
      "2025-12-31 13:29:42,125 - __main__ - ERROR - BiEncoderEvaluator.__init__() missing 2 required positional arguments: 'model_key' and 'test_jsonl'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/awotoroebenezer/Desktop/SenseBackend/src/SR/BI/BiEncoderPipeline.py\", line 169, in evaluate_models\n",
      "    evaluator = BiEncoderEvaluator(\n",
      "                ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: BiEncoderEvaluator.__init__() missing 2 required positional arguments: 'model_key' and 'test_jsonl'\n",
      "2025-12-31 13:29:42,126 - __main__ - INFO - \n",
      "2025-12-31 13:29:42,126 - __main__ - INFO - Evaluated: []\n",
      "2025-12-31 13:29:42,126 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:29:42,126 - __main__ - INFO - VISUALIZATION PHASE\n",
      "2025-12-31 13:29:42,126 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:29:42,126 - __main__ - WARNING - No evaluation results to visualize\n",
      "2025-12-31 13:29:42,126 - src.SR.BI.BiEncoder - INFO - ✓ MPS (Apple Silicon) available\n",
      "2025-12-31 13:29:42,127 - __main__ - INFO - ✓ Summary saved to results/summary.json\n",
      "2025-12-31 13:29:42,127 - __main__ - INFO - \n",
      "2025-12-31 13:29:42,127 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:29:42,127 - __main__ - INFO - ✓ PIPELINE COMPLETED SUCCESSFULLY\n",
      "2025-12-31 13:29:42,127 - __main__ - INFO - ================================================================================\n",
      "2025-12-31 13:29:42,127 - __main__ - INFO - Results saved to: results\n",
      "2025-12-31 13:29:42,127 - __main__ - INFO - Summary: results/summary.json\n",
      "2025-12-31 13:29:42,127 - __main__ - INFO - Visualizations: results/visualizations/index.html\n",
      "2025-12-31 13:29:42,127 - __main__ - INFO - Pipeline log: pipeline.log\n"
     ]
    }
   ],
   "source": [
    "!python -m src.SR.BI.BiEncoderPipeline \\\n",
    "    --qa-train qa_dataset/train/train.jsonl \\\n",
    "    --qa-val qa_dataset/validation/validation.jsonl \\\n",
    "    --qa-test qa_dataset/test/test.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f01cab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8665\n",
      "Model: bert-base-cased\n",
      "[1] (bert-base-cased) PubMedBERT is trained on biomedical text: 0.9108\n",
      "[2] (bert-base-cased) \n",
      "        Title: Global Prevalence of Hypertension: A Systematic Review\n",
      "\n",
      "        Methods:\n",
      "        We searched PubMed, Scopus, and Web of Science from January 1990 to March 2024. \n",
      "        After removing duplicates, 1,240 records were screened, and 137 full-text articles were assessed for eligibility.\n",
      "        Seventy articles were excluded due to incomplete data, leaving 67 studies for analysis. \n",
      "        Two reviewers independently screened all titles and abstracts.\n",
      "\n",
      "        Results:\n",
      "        A total of 67 studies (n = 88,345 participants) from 25 countries were included. \n",
      "        The largest sample sizes were from China (n = 35,812), the United States (n = 10,437), and Germany (n = 7,670). \n",
      "        Other notable sample sizes included Bangladesh (n = 2,909), Ethiopia (n = 2,033), Egypt (n = 3,204), and Vietnam (n = 2,945).\n",
      "        Fourteen were randomized controlled trials, ten cohort studies, and three case-control studies.\n",
      "\n",
      "        Discussion:\n",
      "        The global prevalence of hypertension remains high, particularly in low- and middle-income countries.\n",
      "\n",
      "    : 0.8959\n",
      "[3] (bert-base-cased) Antibiotic resistance is a global health threat: 0.8761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from src.SR.BI.BiEncoderInference import BiEncoderInference\n",
    "\n",
    "# Load a trained model\n",
    "model = BiEncoderInference(\n",
    "    model_dir=Path(\"results/models/bert-base-cased\"),\n",
    "    model_name=\"bert-base-cased\"\n",
    ")\n",
    "\n",
    "# Set corpus\n",
    "documents = [\n",
    "    \"\"\"\n",
    "        Title: Global Prevalence of Hypertension: A Systematic Review\n",
    "\n",
    "        Methods:\n",
    "        We searched PubMed, Scopus, and Web of Science from January 1990 to March 2024. \n",
    "        After removing duplicates, 1,240 records were screened, and 137 full-text articles were assessed for eligibility.\n",
    "        Seventy articles were excluded due to incomplete data, leaving 67 studies for analysis. \n",
    "        Two reviewers independently screened all titles and abstracts.\n",
    "\n",
    "        Results:\n",
    "        A total of 67 studies (n = 88,345 participants) from 25 countries were included. \n",
    "        The largest sample sizes were from China (n = 35,812), the United States (n = 10,437), and Germany (n = 7,670). \n",
    "        Other notable sample sizes included Bangladesh (n = 2,909), Ethiopia (n = 2,033), Egypt (n = 3,204), and Vietnam (n = 2,945).\n",
    "        Fourteen were randomized controlled trials, ten cohort studies, and three case-control studies.\n",
    "\n",
    "        Discussion:\n",
    "        The global prevalence of hypertension remains high, particularly in low- and middle-income countries.\n",
    "\n",
    "    \"\"\",\n",
    "    \"PubMedBERT is trained on biomedical text\",\n",
    "    \"Antibiotic resistance is a global health threat\",\n",
    "]\n",
    "model.set_corpus(documents)\n",
    "\n",
    "# Single similarity\n",
    "result = model.similarity(\n",
    "    query=\"WHat is the total number of studies\",\n",
    "    context=\"Global Prevalence of Hypertension: A Systematic Review\"\n",
    ")\n",
    "print(f\"Score: {result.score:.4f}\")\n",
    "print(f\"Model: {result.model}\")\n",
    "\n",
    "# Retrieve top-3 documents\n",
    "results = model.retrieve(\n",
    "    query=\"WHat is the total number of studies?\",\n",
    "    top_k=3\n",
    ")\n",
    "for r in results:\n",
    "    print(f\"[{r.rank}] ({r.model}) {r.context}: {r.score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7a74ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/awotoroebenezer/Desktop/SenseBackend/.venv_/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:src.SR.paper_extractor:Loading BiEncoder from results/models/bert-base-cased\n",
      "INFO:src.SR.BI.BiEncoder:✓ MPS (Apple Silicon) available\n",
      "INFO:src.SR.BI.BiEncoderInference:Loading model: bert-base-cased\n",
      "INFO:src.SR.BI.BiEncoderInference:  Path: results/models/bert-base-cased\n",
      "INFO:src.SR.BI.BiEncoderInference:  Device: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: results/models/bert-base-cased\n",
      "INFO:src.SR.BI.BiEncoderInference:  Embedding dimension: 768\n",
      "INFO:src.SR.paper_extractor:Building semantic tag index...\n",
      "INFO:src.SR.paper_extractor:Encoding 272 keywords...\n",
      "Batches: 100%|██████████| 9/9 [00:00<00:00, 15.05it/s]\n",
      "INFO:src.SR.paper_extractor:✓ Built index with 272 semantic tags\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from src.Commands.regexp import searchRegEx\n",
    "from src.SR.paper_extractor import PaperExtractor\n",
    "\n",
    "# 2. Sample Paper Text (Abstract)\n",
    "sample_text = \"\"\"\n",
    "Background: We conducted a systematic review to assess the effectiveness of COVID-19 vaccines in adolescents.\n",
    "Methods: We searched PubMed until January 15 2024. We included RCTs and observational studies.\n",
    "Results: The study included 5000 participants. The effectiveness was 95% against infection. \n",
    "Mortality rates were low. No severe adverse events were reported in immunocompromised patients.\n",
    "The study was conducted across Germany and USA.\n",
    "\"\"\"\n",
    "# Your FULL searchRegEx (500+ keywords) now works!\n",
    "extractor = PaperExtractor(\n",
    "    model_path=Path(\"results/models/bert-base-cased\"),\n",
    "    search_definitions=searchRegEx,  # Your complete dict\n",
    "    threshold=0.65\n",
    ")\n",
    "\n",
    "# # Extract from paper\n",
    "# data = extractor.extract(sample_text)\n",
    "\n",
    "# # Get clean JSON\n",
    "# print(json.dumps(data, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299efe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.SR.paper_extractor:Starting extraction...\n",
      "INFO:src.SR.paper_extractor:Found 8 sentences\n",
      "INFO:src.SR.paper_extractor:Encoding sentences...\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.43it/s]\n",
      "INFO:src.SR.paper_extractor:Computing semantic similarities...\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.25it/s]\n",
      "INFO:src.SR.paper_extractor:Matching tags (threshold=0.65)...\n",
      "INFO:src.SR.paper_extractor:Matched 272 tags\n",
      "INFO:src.SR.paper_extractor:✓ Extraction complete\n"
     ]
    }
   ],
   "source": [
    "data = extractor.extract(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42ffb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"date of last literature search\": \"January 15 2024\",\n",
      "  \"number of studies\": \"5000\",\n",
      "  \"Population\": {\n",
      "    \"age__group\": [\n",
      "      \"nb\",\n",
      "      \"chi\",\n",
      "      \"ado\",\n",
      "      \"adu\",\n",
      "      \"eld\"\n",
      "    ],\n",
      "    \"specific__group\": [\n",
      "      \"hcw\",\n",
      "      \"pw\",\n",
      "      \"tra\",\n",
      "      \"cg\"\n",
      "    ],\n",
      "    \"immune__status\": [\n",
      "      \"imu\",\n",
      "      \"hty\"\n",
      "    ]\n",
      "  },\n",
      "  \"topic\": {\n",
      "    \"eff\": [\n",
      "      \"eff\"\n",
      "    ],\n",
      "    \"safety\": [\n",
      "      \"saf\"\n",
      "    ],\n",
      "    \"risk__factor\": [\n",
      "      \"rf\"\n",
      "    ],\n",
      "    \"coverage\": [\n",
      "      \"cov\"\n",
      "    ],\n",
      "    \"acceptance\": [\n",
      "      \"kaa\"\n",
      "    ],\n",
      "    \"adm\": [\n",
      "      \"adm\"\n",
      "    ],\n",
      "    \"eco\": [\n",
      "      \"eco\"\n",
      "    ],\n",
      "    \"modeling\": [\n",
      "      \"mod\"\n",
      "    ],\n",
      "    \"ethical__issues\": [\n",
      "      \"eth\"\n",
      "    ]\n",
      "  },\n",
      "  \"outcome\": {\n",
      "    \"infection\": [\n",
      "      \"inf\"\n",
      "    ],\n",
      "    \"hospital\": [\n",
      "      \"hos\"\n",
      "    ],\n",
      "    \"icu\": [\n",
      "      \"icu\"\n",
      "    ],\n",
      "    \"death\": [\n",
      "      \"dea\"\n",
      "    ]\n",
      "  },\n",
      "  \"intervention\": {\n",
      "    \"vpd\": [\n",
      "      \"covid\",\n",
      "      \"aden\",\n",
      "      \"anth\",\n",
      "      \"camp\",\n",
      "      \"chol\",\n",
      "      \"coxi\",\n",
      "      \"diph\",\n",
      "      \"ebol\",\n",
      "      \"ente\",\n",
      "      \"esch\",\n",
      "      \"hib\",\n",
      "      \"ha\",\n",
      "      \"hb\",\n",
      "      \"hc\",\n",
      "      \"he\",\n",
      "      \"hs\",\n",
      "      \"hz\",\n",
      "      \"hiv\",\n",
      "      \"hpv\",\n",
      "      \"je\",\n",
      "      \"leis\",\n",
      "      \"lyme\",\n",
      "      \"mala\",\n",
      "      \"meas\",\n",
      "      \"meni\",\n",
      "      \"mump\",\n",
      "      \"myle\",\n",
      "      \"myva\",\n",
      "      \"pert\",\n",
      "      \"plag\",\n",
      "      \"pneu\",\n",
      "      \"poli\",\n",
      "      \"pseu\",\n",
      "      \"rabi\",\n",
      "      \"rsv\",\n",
      "      \"rube\",\n",
      "      \"salm\",\n",
      "      \"shig\",\n",
      "      \"smal\",\n",
      "      \"strb\",\n",
      "      \"tt\",\n",
      "      \"tbe\",\n",
      "      \"tb\",\n",
      "      \"typh\",\n",
      "      \"vari\",\n",
      "      \"yf\",\n",
      "      \"zika\",\n",
      "      \"infl\",\n",
      "      \"deng\",\n",
      "      \"rota\"\n",
      "    ],\n",
      "    \"vaccine__options\": [\n",
      "      \"live\",\n",
      "      \"nonlive\",\n",
      "      \"adjuvants\",\n",
      "      \"nonadjuvants\",\n",
      "      \"quad\",\n",
      "      \"4vHPV\",\n",
      "      \"biva\",\n",
      "      \"2vHPV\"\n",
      "    ]\n",
      "  },\n",
      "  \"study_country\": {\n",
      "    \"Germany\": 1,\n",
      "    \"USA\": 1\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8032c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
